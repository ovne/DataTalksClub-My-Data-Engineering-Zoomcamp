{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos pensar que trabalhos de ingestão são uma forma particular de ETL. E para tornar as coisas ainda mais simples, na pratica, faremos um programa em python com a intenção de:\n",
    "- extrair dados de alguma fonte\n",
    "- realizar algum processamento necessário nos dados e por fim (de acordo como objetivo dessa aula do modulo)\n",
    "- fazer o upload desses dados no nosso container servidor de PostgreSQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_20256\\3974309619.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# dependencias\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A etapa de extração não chega a ser um grande desafio, pois, vamos utilizar os arquivos .csv hospedados no github do zoompcamp.\n",
    "\n",
    "https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/yellow\n",
    "\n",
    "Nesse sentido, o método read_csv do pandas vai ser muito conveniente para nós por ser capaz de ler hospedados online a partir da URL e também lidar com a descompressão. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   VendorID               1000 non-null   int64  \n",
      " 1   tpep_pickup_datetime   1000 non-null   object \n",
      " 2   tpep_dropoff_datetime  1000 non-null   object \n",
      " 3   passenger_count        1000 non-null   int64  \n",
      " 4   trip_distance          1000 non-null   float64\n",
      " 5   RatecodeID             1000 non-null   int64  \n",
      " 6   store_and_fwd_flag     1000 non-null   object \n",
      " 7   PULocationID           1000 non-null   int64  \n",
      " 8   DOLocationID           1000 non-null   int64  \n",
      " 9   payment_type           1000 non-null   int64  \n",
      " 10  fare_amount            1000 non-null   float64\n",
      " 11  extra                  1000 non-null   float64\n",
      " 12  mta_tax                1000 non-null   float64\n",
      " 13  tip_amount             1000 non-null   float64\n",
      " 14  tolls_amount           1000 non-null   float64\n",
      " 15  improvement_surcharge  1000 non-null   float64\n",
      " 16  total_amount           1000 non-null   float64\n",
      " 17  congestion_surcharge   0 non-null      float64\n",
      "dtypes: float64(9), int64(6), object(3)\n",
      "memory usage: 140.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Vamos ler as primeiras 1000 linhas para conhecer o dataset e inferir os tipos de dados\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-01.csv.gz\",\n",
    "    compression=\"gzip\",\n",
    "    nrows=1000\n",
    ")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbengine = create_engine('postgresql://postgres:admin@localhost:5432')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TEXT, \n",
      "\ttpep_dropoff_datetime TEXT, \n",
      "\tpassenger_count BIGINT, \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" BIGINT, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=dbengine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputar o schema da tabela no banco de dados\n",
    "df.head(n=0).to_sql(name='yellow_taxi_data', con=dbengine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_chunk = 100000\n",
    "\n",
    "df_iter = pd.read_csv(\n",
    "    filepath_or_buffer=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-01.csv.gz\",\n",
    "    compression=\"gzip\",\n",
    "    iterator=True,\n",
    "    chunksize=iter_chunk\n",
    ")\n",
    "\n",
    "# Para fins de demonstração, vamos carregar apenas 1 milhão de linhas (10 chunks de 100 mil)\n",
    "for i in range(10):\n",
    "    try:\n",
    "        df_chunk = next(df_iter)\n",
    "        df_chunk.tpep_pickup_datetime = pd.to_datetime(df_chunk.tpep_pickup_datetime)\n",
    "        df_chunk.tpep_dropoff_datetime = pd.to_datetime(df_chunk.tpep_dropoff_datetime)\n",
    "        df_chunk.to_sql(name='yellow_taxi_data', con=dbengine, if_exists='append', index=False)\n",
    "        print(f\"> {i}00.000 linhas carregadas para a tabela yellow_taxi_data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Processamento finalizado com erro: {e}\")\n",
    "    finally:\n",
    "        print(f\"> {i}00.000 linhas carregadas para a tabela yellow_taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
